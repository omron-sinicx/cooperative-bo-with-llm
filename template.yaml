theme: default # default || dark
organization: OMRON SINIC X
twitter: '@omron_sinicx'
title: 'Cooperative Design Optimization through Natural Language Interaction'
conference: UIST2025
resources:
  paper: https://arxiv.org/abs/2508.16077
  video: https://youtu.be/Haf2ftUvKUw?si=_Qq0YxEM1lF_6ZC9
description: In this work, we propose the concept of cooperative design optimization through natural language. To realize this, we present a novel technique to integrate an LLM with the Bayesian Optimization (BO)-based optimization procedure, enabling designers to intervene through natural language and understand the systemâ€™s intentions behind its suggestions.
image: https://omron-sinicx.github.io/${your-repository-name}/teaser.png
url: https://omron-sinicx.github.io/${your-repository-name}
authors:
  - name: Ryogo Niwa*
    affiliation: [1, 2]
    url: https://www.niwa-festival.com/
    position: intern
  - name: Shigeo Yoshida
    affiliation: [1]
    url: https://shigeodayo.me/
    position: Researcher
  - name: Yuki Koyama
    affiliation: [3, 4]
    url: https://koyama.xyz/
    position: Researcher
  - name: Yoshitaka Ushiku
    affiliation: [1]
    url: https://yoshitakaushiku.net/index.html
    position: Researcher
contact_ids: ['github', 'omron', 2] #=> github issues, contact@sinicx.com, 2nd author
affiliations:
  - OMRON SINIC X Corporation
  - University of Tsukuba
  - National Institute of Advanced Industrial Science and Technology (AIST)
  - University of Tokyo
meta:
  - '* work done as an intern at OMRON SINIC X.'
bibtex: >
  # arXiv version

header:
  bg_curve:
    sinic_curve.png
    #  bg_image: https://github.com/omron-sinicx/swarm-body/raw/main/images/teaser.jpg

teaser: teaser.png
overview: |
  Designing successful interactions requires identifying optimal design parameters. To do so, designers often conduct iterative user testing and exploratory trial-and-error. This involves balancing multiple objectives in a high-dimensional space, making the process time-consuming and cognitively demanding. System-led optimization methods, such as those based on Bayesian optimization, can determine for designers which parameters to test next. However, they offer limited opportunities for designers to intervene in the optimization process, negatively impacting the designerâ€™s experience. We propose a design optimization framework that enables natural language interactions between designers and the optimization system, facilitating cooperative design optimization. This is achieved by integrating system-led optimization methods with Large Language Models (LLMs), allowing designers to intervene in the optimization process and better understand the system's reasoning. Experimental results show that our method provides higher user agency than a system-led method and shows promising optimization performance compared to manual design. It also matches the performance of an existing cooperative method with lower cognitive load.

body:
  - title: Method
    text: |
      We integrate an LLM into the BO process to realize cooperative design optimization through natural language interaction. In each iteration, the system selects a single point (i.e., a set of parameter values) to be evaluated next with the help of an LLM from multiple candidates generated by a batch BO technique. In this way, we can integrate the designerâ€™s request into the optimization process. Please refer to the following figure for an overview of the system procedure.

      <img src="method.png" alt="On the left side, there is a heatmap-like visualization of the acquisition function for a batch Bayesian optimization, ranging from blue at the top-left to yellow on the right. Warmer (yellow) areas indicate higher acquisition function values, and multiple candidate points are sampled from these high-value regions. These points are marked as circles on the heatmap, with arrows connecting them to a central 'Prompt for LLM' panel that displays task information, user requests, and lists of both previous evaluations and new candidate parameters. On the far right, a set of five sliders labeled x1 through x5 indicates the selected parameters, alongside a short text bubble labeled 'Reason,' which explains the rationale behind these chosen settings." />

      In each step, our system first samples ð‘ž candidates (ð‘ž = 8 in this figure) using a technique called BO. Then, the LLM receives a prompt consisting of the task information, the designerâ€™s request, the previously evaluated parameter-performance pairs, and the predicted performance of the candidates. Finally, it chooses the parameter set that best meets the designerâ€™s request and provides its reason for that choice in natural language.
  - title: Acknowledgments
    text: >
      This work was supported by JST Moonshot R&D Program Grant Number JPMJMS2236.
